{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Partycjonowanie danych w Sparku\n",
    "\n",
    "Partycjonowanie danych jest kluczową techniką zarządzania i optymalizacji przetwarzania dużych zbiorów danych, szczególnie w środowiskach Big Data, takich jak Apache Spark. \n",
    "Polega ono na podziale dużego zbioru danych na mniejsze, zarządzalne części, zwane partycjami. \n",
    "Każda partycja może być przetwarzana niezależnie od innych, co umożliwia równoległe i rozproszone przetwarzanie na wielu węzłach klastra.\n",
    "\n",
    "## Cele partycjonowania:\n",
    "1. Zwiększenie wydajności: Przetwarzając dane w mniejszych blokach równolegle, możemy znacząco przyspieszyć czas przetwarzania zadań.\n",
    "2. Optymalizacja zapytań: Partycjonowanie pozwala na fizyczne grupowanie danych w sposób, który jest zgodny z najczęściej wykonywanymi zapytaniami. Na przykład, partycjonowanie danych według daty pozwala na szybkie wykonywanie zapytań dotyczących określonych okresów czasu, bez konieczności przeszukiwania całego zbioru danych.\n",
    "3. Efektywność zarządzania danymi: Partycjonowanie ułatwia zarządzanie danymi, umożliwiając na przykład łatwiejsze archiwizowanie starych danych czy usunięcie niepotrzebnych partycji bez wpływu na resztę zbioru danych.\n",
    "\n",
    "## Typy partycjonowania:\n",
    "1. Partycjonowanie poziome (Partitioning): Dane są dzielone w oparciu o wartość jednego lub więcej kolumn. Na przykład, dane mogą być partycjonowane po kolumnie daty lub regionu. \n",
    "   W Apache Spark każda partycja jest przypisana do różnych węzłów w klastrze, co umożliwia równoległe przetwarzanie.\n",
    "2. Partycjonowanie pionowe (Sharding): Polega na podziale kolumn tabeli na różne serwery. Każdy serwer przechowuje tylko określoną grupę kolumn, co może być przydatne w bazach danych, gdzie różne kolumny są często używane przez różne zapytania.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a96d1b073d3dc2f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:38.499993Z",
     "start_time": "2024-04-28T11:12:38.496721Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1094a0e10>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.0.94:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>SparkPartitioning</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obiekt sesji zwykle ma nazwę \"spark\"\n",
    "spark = SparkSession.builder.appName(\"SparkPartitioning\").getOrCreate() \n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:38.966207Z",
     "start_time": "2024-04-28T11:12:38.937052Z"
    }
   },
   "id": "874ae983d58ae2a2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "137766"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from os import listdir\n",
    "\n",
    "def list_parquet_files(folder: str) -> list[str]:\n",
    "    return [file for file in listdir(folder) if file.endswith(\".parquet\")]\n",
    "\n",
    "def read_parquet_files(folder: str) -> list[DataFrame]:\n",
    "    files = list_parquet_files(folder)\n",
    "    return [spark.read.parquet(f\"{folder}/{file}\") for file in files]\n",
    "\n",
    "def read_parquet_folder(parquet_files_folder: str) -> DataFrame:\n",
    "    dfs = read_parquet_files(parquet_files_folder)\n",
    "    df = dfs[0]\n",
    "    for i in range(1, len(dfs)):\n",
    "        df = df.union(dfs[i])\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:21.688114Z",
     "start_time": "2024-04-28T11:37:21.441564Z"
    }
   },
   "id": "1dc39cd7e13be13c",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "137766"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = spark.read.parquet(\"../../data/sklep/orders\")\n",
    "orders\n",
    "orders.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:35:32.836021Z",
     "start_time": "2024-04-28T11:35:32.620078Z"
    }
   },
   "id": "86121c0769e7b3",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+---------------+\n",
      "|order_id|   order_date|order_customer_id|   order_status|\n",
      "+--------+-------------+-----------------+---------------+\n",
      "|       1|1374735600000|            11599|         CLOSED|\n",
      "|       2|1374735600000|              256|PENDING_PAYMENT|\n",
      "|       3|1374735600000|            12111|       COMPLETE|\n",
      "|       4|1374735600000|             8827|         CLOSED|\n",
      "|       5|1374735600000|            11318|       COMPLETE|\n",
      "+--------+-------------+-----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:05.587762Z",
     "start_time": "2024-04-28T10:10:05.524741Z"
    }
   },
   "id": "9e172b3b41c922a7",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|order_id|   order_date|\n",
      "+--------+-------------+\n",
      "|       1|1374735600000|\n",
      "|       2|1374735600000|\n",
      "|       3|1374735600000|\n",
      "|       4|1374735600000|\n",
      "|       5|1374735600000|\n",
      "+--------+-------------+\n"
     ]
    }
   ],
   "source": [
    "df = orders.select(\"order_id\", \"order_date\")\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:06.170906Z",
     "start_time": "2024-04-28T10:10:06.110927Z"
    }
   },
   "id": "fb9dac91cfc1872",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:07.096559Z",
     "start_time": "2024-04-28T10:10:07.093844Z"
    }
   },
   "id": "df472ec29dc0f247",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+\n",
      "|order_id|   order_date|          timestamp|\n",
      "+--------+-------------+-------------------+\n",
      "|       1|1374735600000|2013-07-25 09:00:00|\n",
      "|       2|1374735600000|2013-07-25 09:00:00|\n",
      "|       3|1374735600000|2013-07-25 09:00:00|\n",
      "|       4|1374735600000|2013-07-25 09:00:00|\n",
      "|       5|1374735600000|2013-07-25 09:00:00|\n",
      "|       6|1374735600000|2013-07-25 09:00:00|\n",
      "|       7|1374735600000|2013-07-25 09:00:00|\n",
      "|       8|1374735600000|2013-07-25 09:00:00|\n",
      "|       9|1374735600000|2013-07-25 09:00:00|\n",
      "|      10|1374735600000|2013-07-25 09:00:00|\n",
      "|      11|1374735600000|2013-07-25 09:00:00|\n",
      "|      12|1374735600000|2013-07-25 09:00:00|\n",
      "|      13|1374735600000|2013-07-25 09:00:00|\n",
      "|      14|1374735600000|2013-07-25 09:00:00|\n",
      "|      15|1374735600000|2013-07-25 09:00:00|\n",
      "|      16|1374735600000|2013-07-25 09:00:00|\n",
      "|      17|1374735600000|2013-07-25 09:00:00|\n",
      "|      18|1374735600000|2013-07-25 09:00:00|\n",
      "|      19|1374735600000|2013-07-25 09:00:00|\n",
      "|      20|1374735600000|2013-07-25 09:00:00|\n",
      "+--------+-------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"timestamp\", F.from_unixtime(F.col(\"order_date\") / 1000))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:07.726838Z",
     "start_time": "2024-04-28T10:10:07.646671Z"
    }
   },
   "id": "eb547e9be1ed6b98",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-----------+\n",
      "|order_id|   order_date|          timestamp|date_string|\n",
      "+--------+-------------+-------------------+-----------+\n",
      "|       1|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       2|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       3|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       4|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       5|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       6|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       7|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       8|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|       9|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      10|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      11|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      12|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      13|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      14|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      15|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      16|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      17|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      18|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      19|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "|      20|1374735600000|2013-07-25 09:00:00| 2013-07-25|\n",
      "+--------+-------------+-------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"date_string\", F.date_format(\"timestamp\", \"yyyy-MM-dd\"))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:08.539486Z",
     "start_time": "2024-04-28T10:10:08.474892Z"
    }
   },
   "id": "dbe6fca95d2e50df",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:09.167002Z",
     "start_time": "2024-04-28T10:10:09.164274Z"
    }
   },
   "id": "dace387d0f78a3c3",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"date_string\").mode(\"overwrite\").parquet(\"orders_partitioned\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:13.995303Z",
     "start_time": "2024-04-28T10:10:10.087116Z"
    }
   },
   "id": "c79a5136610d05cd",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partycjonowanie po parze year-month"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12eed42ed884254f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-----------+----------+\n",
      "|order_id|   order_date|          timestamp|date_string|year_month|\n",
      "+--------+-------------+-------------------+-----------+----------+\n",
      "|       1|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|\n",
      "|       2|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|\n",
      "|       3|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|\n",
      "|       4|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|\n",
      "|       5|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|\n",
      "+--------+-------------+-------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"year_month\", F.concat(F.year(\"timestamp\"), F.lit(\"-\"), F.format_string(\"%02d\", F.month(\"timestamp\"))))\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:14.074817Z",
     "start_time": "2024-04-28T10:10:13.996152Z"
    }
   },
   "id": "1ccf62efc7ee109d",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year_month\").mode(\"overwrite\").parquet(\"orders_partitioned_ym\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29382dfa298c2ee1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Partycjonowanie po dwóch kolumnach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6800bc6e07bac60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = df.withColumn(\"year\", F.year(\"timestamp\"))\n",
    "df = df.withColumn(\"month\", F.month(\"timestamp\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:14.854147Z",
     "start_time": "2024-04-28T10:10:14.836893Z"
    }
   },
   "id": "fdc50680f593d7a1",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-----------+----------+----+-----+\n",
      "|order_id|   order_date|          timestamp|date_string|year_month|year|month|\n",
      "+--------+-------------+-------------------+-----------+----------+----+-----+\n",
      "|       1|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|2013|    7|\n",
      "|       2|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|2013|    7|\n",
      "|       3|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|2013|    7|\n",
      "|       4|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|2013|    7|\n",
      "|       5|1374735600000|2013-07-25 09:00:00| 2013-07-25|   2013-07|2013|    7|\n",
      "+--------+-------------+-------------------+-----------+----------+----+-----+\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:16.207911Z",
     "start_time": "2024-04-28T10:10:16.141244Z"
    }
   },
   "id": "b7da047968a3a3da",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(\"orders_partitioned_ym2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T10:10:17.733822Z",
     "start_time": "2024-04-28T10:10:17.113555Z"
    }
   },
   "id": "ac5ca5e370568195",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.write.mode(\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345b7c1e98074c7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
